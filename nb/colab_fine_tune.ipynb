{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yyKqhlLslRbS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5T68Mzdl0Et",
        "outputId": "9866835a-5aae-4085-88da-ca551cc02d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.7: Fast Qwen3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "base_model =  \"unsloth/Qwen3-1.7B\"\n",
        "lora_model = \"qwen3-ghalib-lora\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model,\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "eLb7YPFmT6v6"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5WvM2pTmVqs",
        "outputId": "62145931-f56c-44ea-9580-041a7cbb8102"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'Generate a poetic couplet in English inspired by Mirza Ghalib, based on the given theme and sentiment. Provide reasoning.',\n",
              " 'input': 'Theme: eternity\\nSentiment: tragic',\n",
              " 'output': 'Reasoning: Ghalib would view eternity through a tragic lens, finding beauty in contradiction.\\nPoem: Even life seemed a tale we asked in despair,\\nWithout you, all felt broken, beyond repair.'}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"english_reasoning_dataset.jsonl\")[\"train\"]\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "-r-_KorhOo-f",
        "outputId": "69fe9da9-e0cc-4dc6-82a8-24f57b5f1187"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|im_start|>user\\nGenerate a poetic couplet in English inspired by Mirza Ghalib, based on the given theme and sentiment. Provide reasoning.\\nTheme: eternity\\nSentiment: tragic<|im_end|>\\n<|im_start|>assistant\\n<think>\\n Ghalib would view eternity through a tragic lens, finding beauty in contradiction.\\n</think>\\n\\nEven life seemed a tale we asked in despair,\\nWithout you, all felt broken, beyond repair.<|im_end|>\\n'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def generate_conversation(example):\n",
        "\n",
        "    input = f\"{example['instruction']}\\n{example['input']}\"\n",
        "    output = example[\"output\"]\n",
        "    output = output.replace(\"Reasoning:\", \"<think>\")\n",
        "    output = output.replace(\"Poem: \", \"</think>\")\n",
        "\n",
        "    return {\n",
        "        \"conversation\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": input\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": output\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "conversations = dataset.map(generate_conversation)[\"conversation\"]\n",
        "conversations = tokenizer.apply_chat_template(\n",
        "    conversations,\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "conversations[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ppNoNrblSTh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.Series(conversations)\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f2fb3805dbd445528cf4345e07395756",
            "34af6232eeca46d88cf397d879c94072",
            "e5a8af89c9e546ca9223f3dc800e7bf2",
            "dbd2adcd83a64dc9b5c4dbb2dc7c1599",
            "bcfe7d954b1c484886f5c29575555edc",
            "f4c0df83b2384a6499de85f607000986",
            "4a0793a91e7d4648b44e36f49380eccb",
            "1c86f32c1ff84680a755941ec98d70d1",
            "2820961f9a484a3897f586842122707b",
            "9edcdded89324f3ab1477266fcfc07bb",
            "1a0fe851a4ae41e9b37f8c2a5e78004b"
          ]
        },
        "id": "lwTRDkBWpGMC",
        "outputId": "1b469208-7a93-408a-8b04-0ee8c6a42aab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2fb3805dbd445528cf4345e07395756",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = combined_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 100,\n",
        "        learning_rate = 2e-5, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ai8LZZIppKeA",
        "outputId": "7d2f3483-467b-44ad-b97c-e93ba844f6a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 250\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 34,865,152/7,000,000,000 (0.50% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 06:57, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.663900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.565300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.497400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.397100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.366700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.425300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.387400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.371500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.366900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.285400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.316200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.292000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.286900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.250300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.263500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.274100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.232200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.195800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.090200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.069600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.087900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.065600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.134400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.065000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.994300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.082000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.951900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.921000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.949300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.056300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.922800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.943300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.976300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.844200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.915000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.843500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.886600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.812700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.868800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.801000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.809800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.845900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.864300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.772100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.816600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.750400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.696100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.676500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.704400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.692000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.649700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.703300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.763800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.730700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.662200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.678500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.633100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.639500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.551600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.526200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.645800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.527400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.504800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.491800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.491200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.504600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.499800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.460700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.454400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.407400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.442900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.429200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.365900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.366000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.411200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.374500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.389600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.380200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.358000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.350300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.353000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.362000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.308300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.326700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.318800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.345600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.269800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.275000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.311700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.252300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.227200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.210500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.221300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.218800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.219200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.227200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.219900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.225600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.190100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.218200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.170100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.192300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.188100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.193900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.179800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.184400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.190600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.186600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.183900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.172100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.172600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.168100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.168200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.163600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.164600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.154300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.155300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.167800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.172100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.188200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.149100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.157800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.145500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.160200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.145500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.163600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.143500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.141400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.149200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.139300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.153100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.160300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.160600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.160300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.143500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.158600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.154600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.154600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.136600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.145400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.134400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.135300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.149300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.155500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.146700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.148900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.139100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.143600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.138000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.145400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.138900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.141000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.134900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.145800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.148100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.157700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.141800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.132300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.145200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.141900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.132900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.134600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.153000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.141700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.135900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.144400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.134000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.136700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.147400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.148400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.140100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.129800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.148800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.137900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.134700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.136300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.136700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.136400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.144300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.132100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.134600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.144700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.142300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.145500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.136600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.135800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.135600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.143200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.138800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.131300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.137900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.140700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Train the model\n",
        "trainer.train()\n",
        "\n",
        "#Save the LoRA Adapter\n",
        "trainer.model.save_pretrained(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KQXTaDxU1um",
        "outputId": "2608ba13-ac7f-4408-efb6-9677fa967071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mirza Ghalib's poetry is known for its introspective, melancholic depth, often reflecting on human frailty, love, and loss. In this pensive couplet, I aim to mirror that tone through metaphor and introspection.\n",
            "\n",
            "**Couplet:**\n",
            "\n",
            "Though I may have lost my love,  \n",
            "I still dream of her in the moonlight.  \n",
            "Her eyes still haunt my soul,  \n",
            "A ghost of joy, a shadow of pain.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Generate a poetic couplet in English inspired by Mirza Ghalib, based on the given theme and sentiment. Provide reasoning.\n",
        "Theme: betrayal\n",
        "Sentiment: pensive\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 500, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t0G7qEZVhN7",
        "outputId": "27fa47cd-d8d4-4e0c-981e-d94b5cdbebef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            " Ghalib’s verses are often steeped in existential musings, where personal anguish mirrors broader human frailties. Betrayal, as a theme, is a mirror to Ghalib’s introspection. The sentiment is pensive, revealing a sense of loss and self-doubt. The couplet should reflect these depths, using metaphors drawn from nature and introspection. Ghalib’s language is both poetic and philosophical, blending imagery with emotional resonance. The couplet must balance these elements, ensuring a flow that aligns with Ghalib’s lyrical style.\n",
            "</think>\n",
            "\n",
            "The sun’s last light through the dying tree,\n",
            "Its shadow now seems to be watching me.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Generate a poetic couplet in English inspired by Mirza Ghalib, based on the given theme and sentiment. Provide reasoning.\n",
        "Theme: betrayal\n",
        "Sentiment: pensive\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : prompt}\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = True, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1024, # Increase for longer outputs!\n",
        "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx7yi0BBbXcy",
        "outputId": "ea8fe0f5-13ca-43d2-dcb1-0e5c60ab7ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.53 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:00<00:00, 52.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving fikr-e-ghalib_qwen3-1.7B_lora/pytorch_model.bin...\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at fikr-e-ghalib_qwen3-1.7B_lora into f16 GGUF format.\n",
            "The output location will be /content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: fikr-e-ghalib_qwen3-1.7B_lora\n",
            "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 40960\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 6144\n",
            "INFO:hf-to-gguf:gguf: head count = 16\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- messages[0].content + '\\n\\n' }}\n",
            "    {%- endif %}\n",
            "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
            "{%- for forward_message in messages %}\n",
            "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
            "    {%- set message = messages[index] %}\n",
            "    {%- set tool_start = '<tool_response>' %}\n",
            "    {%- set tool_start_length = tool_start|length %}\n",
            "    {%- set start_of_message = message.content[:tool_start_length] %}\n",
            "    {%- set tool_end = '</tool_response>' %}\n",
            "    {%- set tool_end_length = tool_end|length %}\n",
            "    {%- set start_pos = (message.content|length) - tool_end_length %}\n",
            "    {%- if start_pos < 0 %}\n",
            "        {%- set start_pos = 0 %}\n",
            "    {%- endif %}\n",
            "    {%- set end_of_message = message.content[start_pos:] %}\n",
            "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
            "        {%- set ns.multi_step_tool = false %}\n",
            "        {%- set ns.last_query_index = index %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {%- set content = message.content %}\n",
            "        {%- set reasoning_content = '' %}\n",
            "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
            "            {%- set reasoning_content = message.reasoning_content %}\n",
            "        {%- else %}\n",
            "            {%- if '</think>' in message.content %}\n",
            "                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n",
            "                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n",
            "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
            "            {%- endif %}\n",
            "        {%- endif %}\n",
            "        {%- if loop.index0 > ns.last_query_index %}\n",
            "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
            "            {%- else %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "            {%- endif %}\n",
            "        {%- else %}\n",
            "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "        {%- endif %}\n",
            "        {%- if message.tool_calls %}\n",
            "            {%- for tool_call in message.tool_calls %}\n",
            "                {%- if (loop.first and content) or (not loop.first) %}\n",
            "                    {{- '\\n' }}\n",
            "                {%- endif %}\n",
            "                {%- if tool_call.function %}\n",
            "                    {%- set tool_call = tool_call.function %}\n",
            "                {%- endif %}\n",
            "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
            "                {{- tool_call.name }}\n",
            "                {{- '\", \"arguments\": ' }}\n",
            "                {%- if tool_call.arguments is string %}\n",
            "                    {{- tool_call.arguments }}\n",
            "                {%- else %}\n",
            "                    {{- tool_call.arguments | tojson }}\n",
            "                {%- endif %}\n",
            "                {{- '}\\n</tool_call>' }}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
            "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.F16.gguf: n_tensors = 310, total_size = 3.4G\n",
            "Writing: 100%|██████████| 3.44G/3.44G [00:59<00:00, 57.5Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5335 (d8919424)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.F16.gguf' to '/content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 27 key-value pairs and 310 tensors from /content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Fikr E Ghalib_Qwen3 1.7B_lora\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = 1.7B_lora\n",
            "llama_model_loader: - kv   4:                           general.basename str              = fikr-e-ghalib_qwen3\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 1.7B\n",
            "llama_model_loader: - kv   6:                          qwen3.block_count u32              = 28\n",
            "llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n",
            "llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 6144\n",
            "llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - type  f32:  113 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 310]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 310]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n",
            "[   3/ 310]                  blk.0.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[   4/ 310]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   5/ 310]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 310]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 310]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   8/ 310]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   9/ 310]                  blk.0.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  10/ 310]                blk.0.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  11/ 310]                blk.0.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  12/ 310]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  13/ 310]                  blk.0.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  14/ 310]                  blk.1.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  15/ 310]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  16/ 310]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  17/ 310]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  18/ 310]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  19/ 310]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  20/ 310]                  blk.1.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  21/ 310]                blk.1.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  22/ 310]                blk.1.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  23/ 310]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 310]                  blk.1.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  25/ 310]                  blk.2.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  26/ 310]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  27/ 310]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  28/ 310]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  29/ 310]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  30/ 310]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  31/ 310]                  blk.2.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  32/ 310]                blk.2.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  33/ 310]                blk.2.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  34/ 310]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  35/ 310]                  blk.2.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  36/ 310]                  blk.3.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  37/ 310]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  38/ 310]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  39/ 310]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  40/ 310]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 310]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  42/ 310]                  blk.3.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  43/ 310]                blk.3.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  44/ 310]                blk.3.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  45/ 310]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  46/ 310]                  blk.3.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  47/ 310]                  blk.4.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  48/ 310]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  49/ 310]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 310]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  51/ 310]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  52/ 310]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  53/ 310]                  blk.4.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  54/ 310]                blk.4.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  55/ 310]                blk.4.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  56/ 310]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 310]                  blk.4.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  58/ 310]                  blk.5.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  59/ 310]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  60/ 310]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  61/ 310]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 310]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  63/ 310]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  64/ 310]                  blk.5.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  65/ 310]                blk.5.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  66/ 310]                blk.5.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  67/ 310]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 310]                  blk.5.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  69/ 310]                  blk.6.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  70/ 310]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  71/ 310]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  72/ 310]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  73/ 310]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  74/ 310]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  75/ 310]                  blk.6.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  76/ 310]                blk.6.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  77/ 310]                blk.6.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  78/ 310]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  79/ 310]                  blk.6.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  80/ 310]                  blk.7.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  81/ 310]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  82/ 310]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  83/ 310]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  84/ 310]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  85/ 310]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  86/ 310]                  blk.7.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  87/ 310]                blk.7.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  88/ 310]                blk.7.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  89/ 310]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  90/ 310]                  blk.7.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[  91/ 310]                  blk.8.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[  92/ 310]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  93/ 310]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  94/ 310]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 310]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  96/ 310]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[  97/ 310]                  blk.8.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[  98/ 310]                blk.8.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[  99/ 310]                blk.8.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 100/ 310]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 101/ 310]                  blk.8.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 102/ 310]                  blk.9.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 103/ 310]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 104/ 310]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 310]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 106/ 310]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 310]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 108/ 310]                  blk.9.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 109/ 310]                blk.9.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 110/ 310]                blk.9.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 111/ 310]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 112/ 310]                  blk.9.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 113/ 310]                 blk.10.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 114/ 310]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 115/ 310]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 116/ 310]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 117/ 310]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 118/ 310]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 119/ 310]                 blk.10.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 120/ 310]               blk.10.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 121/ 310]               blk.10.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 122/ 310]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 123/ 310]                 blk.10.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 124/ 310]                 blk.11.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 125/ 310]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 126/ 310]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 127/ 310]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 128/ 310]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 129/ 310]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 130/ 310]                 blk.11.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 131/ 310]               blk.11.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 132/ 310]               blk.11.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 133/ 310]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 134/ 310]                 blk.11.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 135/ 310]                 blk.12.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 136/ 310]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 137/ 310]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 310]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 139/ 310]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 140/ 310]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 141/ 310]                 blk.12.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 142/ 310]               blk.12.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 143/ 310]               blk.12.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 144/ 310]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 145/ 310]                 blk.12.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 146/ 310]                 blk.13.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 147/ 310]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 148/ 310]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 149/ 310]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 150/ 310]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 151/ 310]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 152/ 310]                 blk.13.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 153/ 310]               blk.13.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 154/ 310]               blk.13.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 155/ 310]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 156/ 310]                 blk.13.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 157/ 310]                 blk.14.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 158/ 310]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 159/ 310]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 160/ 310]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 161/ 310]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 310]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 163/ 310]                 blk.14.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 164/ 310]               blk.14.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 165/ 310]               blk.14.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 166/ 310]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 167/ 310]                 blk.14.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 168/ 310]                 blk.15.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 169/ 310]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 170/ 310]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 171/ 310]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 172/ 310]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 173/ 310]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 174/ 310]                 blk.15.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 175/ 310]               blk.15.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 176/ 310]               blk.15.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 177/ 310]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 178/ 310]                 blk.15.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 179/ 310]                 blk.16.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 180/ 310]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 181/ 310]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 182/ 310]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 183/ 310]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 184/ 310]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 185/ 310]                 blk.16.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 186/ 310]               blk.16.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 187/ 310]               blk.16.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 188/ 310]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 189/ 310]                 blk.16.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 190/ 310]                 blk.17.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 191/ 310]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 192/ 310]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 193/ 310]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 194/ 310]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 195/ 310]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 196/ 310]                 blk.17.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 197/ 310]               blk.17.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 198/ 310]               blk.17.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 199/ 310]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 200/ 310]                 blk.17.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 201/ 310]                 blk.18.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 202/ 310]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 203/ 310]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 204/ 310]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 205/ 310]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 206/ 310]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 207/ 310]                 blk.18.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 208/ 310]               blk.18.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 209/ 310]               blk.18.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 210/ 310]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 211/ 310]                 blk.18.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 212/ 310]                 blk.19.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 213/ 310]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 214/ 310]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 215/ 310]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 310]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 217/ 310]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 218/ 310]                 blk.19.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 219/ 310]               blk.19.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 220/ 310]               blk.19.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 221/ 310]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 222/ 310]                 blk.19.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 223/ 310]                 blk.20.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 224/ 310]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 225/ 310]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 226/ 310]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 227/ 310]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 228/ 310]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 229/ 310]                 blk.20.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 230/ 310]               blk.20.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 231/ 310]               blk.20.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 232/ 310]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 233/ 310]                 blk.20.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 234/ 310]                 blk.21.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 235/ 310]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 236/ 310]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 237/ 310]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 238/ 310]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 310]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 240/ 310]                 blk.21.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 241/ 310]               blk.21.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 242/ 310]               blk.21.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 243/ 310]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 244/ 310]                 blk.21.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 245/ 310]                 blk.22.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 246/ 310]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 247/ 310]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 248/ 310]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 249/ 310]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 250/ 310]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 251/ 310]                 blk.22.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 252/ 310]               blk.22.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 253/ 310]               blk.22.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 254/ 310]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 255/ 310]                 blk.22.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 256/ 310]                 blk.23.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 257/ 310]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 258/ 310]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 259/ 310]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 260/ 310]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 261/ 310]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 262/ 310]                 blk.23.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 263/ 310]               blk.23.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 264/ 310]               blk.23.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 265/ 310]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 266/ 310]                 blk.23.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 267/ 310]                 blk.24.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 268/ 310]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 269/ 310]              blk.24.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 270/ 310]            blk.24.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 271/ 310]                 blk.24.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 272/ 310]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 273/ 310]                 blk.24.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 274/ 310]               blk.24.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 275/ 310]               blk.24.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 276/ 310]               blk.24.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 277/ 310]                 blk.24.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 278/ 310]                 blk.25.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 279/ 310]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 280/ 310]              blk.25.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 281/ 310]            blk.25.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 282/ 310]                 blk.25.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 283/ 310]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 284/ 310]                 blk.25.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 285/ 310]               blk.25.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 286/ 310]               blk.25.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 287/ 310]               blk.25.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 288/ 310]                 blk.25.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 289/ 310]                 blk.26.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 290/ 310]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 291/ 310]              blk.26.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 292/ 310]            blk.26.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 293/ 310]                 blk.26.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 294/ 310]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 295/ 310]                 blk.26.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 296/ 310]               blk.26.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 297/ 310]               blk.26.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 298/ 310]               blk.26.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 299/ 310]                 blk.26.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 300/ 310]                 blk.27.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
            "[ 301/ 310]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 302/ 310]              blk.27.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 303/ 310]            blk.27.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 304/ 310]                 blk.27.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 305/ 310]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[ 306/ 310]                 blk.27.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n",
            "[ 307/ 310]               blk.27.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
            "[ 308/ 310]               blk.27.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "[ 309/ 310]               blk.27.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 310/ 310]                 blk.27.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
            "llama_model_quantize_impl: model size  =  3281.97 MB\n",
            "llama_model_quantize_impl: quant size  =  1050.43 MB\n",
            "\n",
            "main: quantize time = 172117.34 ms\n",
            "main:    total time = 172117.34 ms\n",
            "Unsloth: Conversion completed! Output location: /content/fikr-e-ghalib_qwen3-1.7B_lora/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"babanomania/fikr-e-ghalib_qwen3-1.7B_lora\", tokenizer, save_method = \"lora\", token = userdata.get('HF_TOKEN'))\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"babanomania/fikr-e-ghalib_qwen3-1.7B_lora\",\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = userdata.get('HF_TOKEN'),\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a0fe851a4ae41e9b37f8c2a5e78004b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c86f32c1ff84680a755941ec98d70d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2820961f9a484a3897f586842122707b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34af6232eeca46d88cf397d879c94072": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c0df83b2384a6499de85f607000986",
            "placeholder": "​",
            "style": "IPY_MODEL_4a0793a91e7d4648b44e36f49380eccb",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "4a0793a91e7d4648b44e36f49380eccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9edcdded89324f3ab1477266fcfc07bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfe7d954b1c484886f5c29575555edc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd2adcd83a64dc9b5c4dbb2dc7c1599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9edcdded89324f3ab1477266fcfc07bb",
            "placeholder": "​",
            "style": "IPY_MODEL_1a0fe851a4ae41e9b37f8c2a5e78004b",
            "value": " 2000/2000 [00:03&lt;00:00, 760.44 examples/s]"
          }
        },
        "e5a8af89c9e546ca9223f3dc800e7bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c86f32c1ff84680a755941ec98d70d1",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2820961f9a484a3897f586842122707b",
            "value": 2000
          }
        },
        "f2fb3805dbd445528cf4345e07395756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34af6232eeca46d88cf397d879c94072",
              "IPY_MODEL_e5a8af89c9e546ca9223f3dc800e7bf2",
              "IPY_MODEL_dbd2adcd83a64dc9b5c4dbb2dc7c1599"
            ],
            "layout": "IPY_MODEL_bcfe7d954b1c484886f5c29575555edc"
          }
        },
        "f4c0df83b2384a6499de85f607000986": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
